# Video Analysis Pipeline with Vision Language Models

Pipeline for analyzing videos using Vision Language Models (Qwen, InternVL) with vLLM inference.

## Quick Start

### 1. Installation

```bash
# Create virtual environment
pip install uv
uv venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Preprocessing (One-Time Setup)

Extract frames and transcribe audio before inference:

```bash
python preprocess.py data/configs/config_preprocess.json
```

### 3. Run Inference

```bash
# Qwen models
python main.py data/configs/config_qwen.json

# InternVL models
python main.py data/configs/config_internvl.json
```

## Running Different Models

### Qwen Models
```json
{
  "MODEL": {
    "MODEL_NAME": "Qwen/Qwen2.5-VL-32B-Instruct",
    "MODEL_FAMILY": "qwen",
    "MODEL_SUFFIX": "qwenvl_32b",
    "DTYPE": "half",
    "MAX_TOKENS": 2048,
    "TEMPERATURE": 0.1,
    "TOP_P": 0.9,
    "REPETITION_PENALTY": 1.05
  }
}
```

**Available models:**
- `Qwen/Qwen2.5-VL-7B-Instruct`
- `Qwen/Qwen2.5-VL-32B-Instruct`

### InternVL Models
```json
{
  "MODEL": {
    "MODEL_NAME": "OpenGVLab/InternVL2_5-26B",
    "MODEL_FAMILY": "internvl",
    "MODEL_SUFFIX": "internvl_26b",
    "DTYPE": "half",
    "MAX_TOKENS": 2048,
    "TEMPERATURE": 0.1,
    "TOP_P": 0.9,
    "REPETITION_PENALTY": 1.05
  }
}
```

**Available models:**
- `OpenGVLab/InternVL2_5-8B`
- `OpenGVLab/InternVL2_5-26B`

## Running on New Datasets

### 1. Update Config Paths
```json
{
  "PATHS": {
    "VIDEO_FOLDER": "path/to/your/videos/",
    "PROMPT_FILE": "path/to/your/prompt.txt",
    "AUDIO_TRANSCRIPT_FOLDER": "data/inputs/audio_transcripts/",
    "KEY_FRAMES_FOLDER": "data/inputs/key_frames/",
    "UNIFORM_FRAMES_FOLDER": "data/inputs/uniform_frames/",
    "GROUND_TRUTH_FILE": "path/to/ground_truth.csv"  // Optional
  },
  "OUTPUT": {
    "OUTPUT_FOLDER": "data/outputs/your_experiment"
  }
}
```
The ground truth is typically a csv file generated by humans or proprietery models such as Gemini or OpenAI after analyzing the videos. It is used to evaluate the current open source model's performance.

### 2. Update System Prompt
Edit your `PROMPT_FILE` to define what information the model should extract from videos. The current prompt asks the model to generate content certain fields in JSON format. These JSON responses are then converted to a csv file for easier analysis.

### 3. Run Preprocessing
```bash
python preprocess.py config_preprocess.json
```

### 4. Run Inference
```bash
python main.py config_your_model.json
```

## Adding New Models

### 1. Create Model Loader
Create `utils_{model_family}/model_loader.py`:

```python
def load_your_model(config):
    from vllm import LLM, SamplingParams
    
    llm = LLM(
        model=config["MODEL"]["MODEL_NAME"],
        trust_remote_code=True,
        dtype=config["MODEL"]["DTYPE"],
        # Add model-specific parameters
    )
    
    sampling = SamplingParams(
        temperature=config["MODEL"]["TEMPERATURE"],
        top_p=config["MODEL"]["TOP_P"],
        max_tokens=config["MODEL"]["MAX_TOKENS"]
    )
    
    return llm, sampling
```

### 2. Update main.py

Add to `load_model_for_family()`:
```python
elif model_family == 'your_model':
    from utils_your_model.model_loader import load_your_model
    llm, sampling_params = load_your_model(config)
    return llm, None, sampling_params, model_family, True
```

Add to `prepare_multimodal_input()`:
```python
elif model_family == 'your_model':
    return prepare_your_model_input(images, audio_text, system_prompt)
```

Create `prepare_your_model_input()` function:
```python
def prepare_your_model_input(images, audio_text, system_prompt):
    # Format prompt according to your model's requirements
    prompt = f"Your model-specific prompt format with {len(images)} images"
    
    return {
        "prompt": prompt,
        "multi_modal_data": {"image": images}
    }
```

### 3. Create Config File
```json
{
  "MODEL": {
    "MODEL_NAME": "your-org/your-model",
    "MODEL_FAMILY": "your_model",
    "MODEL_SUFFIX": "your_model_suffix",
    "DTYPE": "half",
    "MAX_TOKENS": 2048
  }
}
```

## Configuration Reference

### Core Settings

**PATHS**
- `VIDEO_FOLDER`: Input videos directory
- `PROMPT_FILE`: System instruction prompt
- `AUDIO_TRANSCRIPT_FOLDER`: Audio transcripts location
- `KEY_FRAMES_FOLDER`: Extracted key frames location
- `UNIFORM_FRAMES_FOLDER`: Uniform frames location
- `GROUND_TRUTH_FILE`: Ground truth for evaluation (optional)

**MODEL**
- `MODEL_NAME`: HuggingFace model ID
- `MODEL_FAMILY`: `"qwen"` or `"internvl"`
- `MODEL_SUFFIX`: Short name for output folders
- `DTYPE`: `"half"` (fp16) or `"bfloat16"`
- `MAX_TOKENS`: Maximum generation length
- `TEMPERATURE`: Sampling temperature (0.0-1.0)
- `TOP_P`: Nucleus sampling threshold
- `REPETITION_PENALTY`: Repetition penalty (1.0 = no penalty)

**VIDEO_PROCESSING**
- `NUM_SEGMENTS`: Number of frames per video (recommended: 30)
- `INPUT_SIZE`: Frame resize dimension (recommended: 360 or 224)

**FEATURES**
- `USE_KEY_FRAMES`: Use scene-based key frames (variable count). The keyframes are extracted using scenedetect library.
- `USE_UNIFORM_FRAMES`: Use uniformly sampled frames (fixed count)
- If both false: Extract frames on-the-fly

**SAMPLING**
- `SAMPLE`: Process subset for testing
- `SAMPLE_SIZE`: Number of videos to process

**EVALUATION**
- `CALCULATE_METRICS`: Enable/disable evaluation against ground truth

### Preprocessing Configuration

```json
{
  "FEATURES": {
    "TRANSCRIBE_AUDIO": true,
    "EXTRACT_KEY_FRAMES": false,
    "EXTRACT_UNIFORM_FRAMES": true
  },
  "AUDIO_TRANSCRIPTION": {
    "WHISPER_MODEL": "openai/whisper-large-v3",
    "DEVICE_ID": 0,
    "CHUNK_LENGTH_S": 30,
    "BATCH_SIZE": 16
  },
  "KEY_FRAME_EXTRACTION": {
    "DETECTION_MODE": "scenedetect",
    "MAX_WORKERS": 8,
    "JPEG_QUALITY": 85,
    "THRESHOLD": 30.0,
    "MIN_SCENE_LEN": 15
  }
}
```

## Output Structure

```
data/outputs/{model_suffix}_inputsize{size}_numframes{frames}/
├── json/              # Per-video JSON responses
├── csv/
│   └── response.csv   # Aggregated results
├── statistics/
│   ├── inference_statistics.txt
│   └── evaluation.json
└── missed_videos.txt  # Failed videos with errors
```

## Docker-Based Inference

Use `main_docker.py` when you want to spin up vLLM directly inside a docker container. The helper reads all runtime settings from the default config file `data/configs/config_docker.json`

Docker setup steps:

Download the docker image:
```
docker pull vllm/vllm-openai:latest
```

Sve the docker image as a tar file which can be loaded later.
```
docker save vllm/vllm-openai:latest -o <DOCKER_PATH>/vllm-openai-latest.tar
```


### Running the pipeline

```bash
python main_docker.py data/configs/config_docker.json
```
You can change the config path as needed.

Note: Copy the example config file `data/configs/config_docker_example.json` to `config_docker.json` and update DOCKER_PATH and HF_HOME_PATH in the config file before running.
You can update the model by changing the `SELECTED_KEY` field in the config.

To add a new model, duplicate one of the entries in the `MODEL.CONFIGS` block and modify the parameters as needed. Check the TOOL_CALL_PARSER parameter for the new model.

This will:
1. Load (and optionally extract) the requested Docker image.
2. Start the container with the GPU, port, and Hugging Face cache mappings defined in the config.
3. Wait for the `/v1` endpoint to become healthy, then batch all videos in the configured folders.
4. Save JSON, CSV, evaluation metrics, and a human-readable `inference_statistics.txt` alongside your other outputs.

### Key config sections

```json
{
  "MODEL": {
    "SELECTED_KEY": "qwenvl32b",
    "CONFIGS": {
      "qwenvl32b": {
        "MODEL_NAME": "Qwen/Qwen2.5-VL-32B-Instruct",
        "MODEL_SUFFIX": "qwenvl32b",
        "TENSOR_PARALLEL_SIZE": 2,
        "DTYPE": "bfloat16",
        "REQUEST": { "MAX_TOKENS": 2048, "TEMPERATURE": 0.7 },
        "DOCKER": { "CONTAINER_NAME": "qwenvl32b-vllm", "HOST_PORT": 8800 }
      }
    }
  },
  "STATISTICS": { "SAVE_DETAILED": true }
}
```

- **MODEL.CONFIGS** — Bundle per-model switches (name, dtype, tensor parallelism) so you can change models by editing only `SELECTED_KEY`.
- **MODEL.CONFIGS.*.REQUEST** — Overrides for OpenAI-compatible request params (tokens, temperature, retry settings). Falls back to the top-level `REQUEST` block if omitted.
- **MODEL.CONFIGS.*.DOCKER** — Container details such as image path, container name, GPU flags, and startup sleeps. Shared defaults live in the top-level `DOCKER` block.
- **STATISTICS.SAVE_DETAILED** — Set to `false` to store only aggregate averages/min/max in `inference_statistics.txt`; `true` logs per-video load and inference timings.

Tip: Duplicate the `CONFIGS` entry for each model you need (e.g., `internvl26b`) and point `SELECTED_KEY` to the one you’d like to run. The script automatically reuses a running server if it finds one on the configured port, so you can restart inference without rebooting the container.

### Parameter reference (docker configs)

| Field | Location | Description |
| --- | --- | --- |
| `PATHS.VIDEO_FOLDER` | Top-level | Absolute folder containing the source videos; file names drive `video_id` detection. |
| `PATHS.KEY_FRAMES_FOLDER` / `UNIFORM_FRAMES_FOLDER` | Top-level | Parent folders that hold precomputed frames (`video_id` subfolders). Enable one via `FEATURES`. |
| `PATHS.AUDIO_TRANSCRIPT_FOLDER` | Top-level | Text transcripts (`<video_id>.txt`) that get appended to the system prompt. |
| `OUTPUT.OUTPUT_FOLDER` | Top-level | Base prefix for JSON/CSV/stats directories before the script adds suffixes for input size and frame count. |
| `VIDEO_PROCESSING.NUM_SEGMENTS` | Top-level | Number of frames to expect per video when using uniform extraction; used in folder naming. |
| `VIDEO_PROCESSING.INPUT_SIZE` | Top-level | Square resize dimension (pixels) applied to every frame before base64 encoding. |
| `FEATURES.USE_KEY_FRAMES` / `USE_UNIFORM_FRAMES` | Top-level | Select which precomputed frame strategy to load; at least one must be true. |
| `SAMPLING.SAMPLE` / `SAMPLING.SAMPLE_SIZE` | Top-level | Limit how many videos run in a batch (useful for smoke tests). |
| `MODEL.SELECTED_KEY` | `MODEL` | Chooses which entry in `MODEL.CONFIGS` becomes active for this run. |
| `MODEL.CONFIGS.*.MODEL_NAME` | `MODEL.CONFIGS.<key>` | Hugging Face model to load inside vLLM (e.g., Qwen 32B). |
| `MODEL.CONFIGS.*.MODEL_SUFFIX` | `MODEL.CONFIGS.<key>` | Short label appended to output folders/filenames. |
| `MODEL.CONFIGS.*.TENSOR_PARALLEL_SIZE` | `MODEL.CONFIGS.<key>` | Number of GPUs vLLM will shard the model across. |
| `MODEL.CONFIGS.*.DTYPE` | `MODEL.CONFIGS.<key>` | Precision passed to vLLM (`bfloat16`, `half`, etc.). |
| `MODEL.CONFIGS.*.MAX_MODEL_LEN` | `MODEL.CONFIGS.<key>` | Context window size in tokens for the server. |
| `MODEL.CONFIGS.*.GPU_MEMORY_UTILIZATION` | `MODEL.CONFIGS.<key>` | Fraction of each GPU memory vLLM is allowed to allocate. |
| `MODEL.CONFIGS.*.MAX_NUM_SEQS` | `MODEL.CONFIGS.<key>` | Maximum concurrent sequences the server will batch. |
| `MODEL.CONFIGS.*.MAX_NUM_BATCHED_TOKENS` | `MODEL.CONFIGS.<key>` | Hard cap on total tokens per batch for stability. |
| `MODEL.CONFIGS.*.KV_CACHE_MEMORY_BYTES` | `MODEL.CONFIGS.<key>` | Additional cache budget for attention key/value tensors. |
| `MODEL.CONFIGS.*.TOKENIZER_MODE` / `CONFIG_FORMAT` / `LOAD_FORMAT` | `MODEL.CONFIGS.<key>` | Advanced vLLM switches for tokenizer/model config handling; keep at `auto` unless debugging. |
| `MODEL.CONFIGS.*.TOOL_CALL_PARSER` | `MODEL.CONFIGS.<key>` | Parser used for tool-call outputs (Qwen expects `qwen3_xml`). |
| `MODEL.CONFIGS.*.ENABLE_AUTO_TOOL_CHOICE` | `MODEL.CONFIGS.<key>` | Enables Qwen’s automatic tool selection logic if `true`. |
| `MODEL.CONFIGS.*.REQUEST.MAX_TOKENS` | `MODEL.CONFIGS.<key>.REQUEST` | Per-request generation cap forwarded to `/chat/completions`. |
| `MODEL.CONFIGS.*.REQUEST.TEMPERATURE` | `MODEL.CONFIGS.<key>.REQUEST` | Sampling temperature sent with each request. |
| `MODEL.CONFIGS.*.REQUEST.MAX_RETRIES` / `RETRY_SLEEP_SECONDS` / `TIMEOUT_SECONDS` | `MODEL.CONFIGS.<key>.REQUEST` | Transport-layer retry behavior when the HTTP call fails. |
| `MODEL.CONFIGS.*.DOCKER.IMAGE_TAR` | `MODEL.CONFIGS.<key>.DOCKER` | Optional tarball path used to `docker load` the image before running. |
| `MODEL.CONFIGS.*.DOCKER.IMAGE_NAME` | `MODEL.CONFIGS.<key>.DOCKER` | Docker image reference passed to `docker run`. |
| `MODEL.CONFIGS.*.DOCKER.CONTAINER_NAME` | `MODEL.CONFIGS.<key>.DOCKER` | Name assigned to the running container (used for cleanup/reuse). |
| `MODEL.CONFIGS.*.DOCKER.HOST_PORT` / `CONTAINER_PORT` | `MODEL.CONFIGS.<key>.DOCKER` | External vs. internal HTTP ports for the OpenAI-compatible endpoint. |
| `MODEL.CONFIGS.*.DOCKER.GPUS` | `MODEL.CONFIGS.<key>.DOCKER` | GPU visibility string passed to Docker (`all`, `device=0`, etc.). |
| `MODEL.CONFIGS.*.DOCKER.IPC` | `MODEL.CONFIGS.<key>.DOCKER` | IPC mode (`host` is recommended for large models). |
| `MODEL.CONFIGS.*.DOCKER.HF_HOME` | `MODEL.CONFIGS.<key>.DOCKER` | Host folder mounted to `/root/.cache/huggingface` for caching checkpoints. |
| `MODEL.CONFIGS.*.DOCKER.POST_LOAD_SLEEP_SECONDS` | `MODEL.CONFIGS.<key>.DOCKER` | Grace period after `docker load` to let the registry settle. |
| `MODEL.CONFIGS.*.DOCKER.POST_RUN_SLEEP_SECONDS` | `MODEL.CONFIGS.<key>.DOCKER` | Extra wait after `docker run` before issuing health checks (helpful for huge models). |
| `MODEL.CONFIGS.*.DOCKER.HEALTHCHECK_RETRIES` / `HEALTHCHECK_SLEEP_SECONDS` | `MODEL.CONFIGS.<key>.DOCKER` | How many times (and how often) the script polls `/v1/models` before giving up. |
| `STATISTICS.SAVE_DETAILED` | Top-level | Controls whether every video’s load/inference time is logged or only aggregate stats. |
| `EVALUATION.CALCULATE_METRICS` | Top-level | Enables BERTScore/category evaluation when ground-truth CSVs exist. |

## Docker debugging steps

- Sometimes the docker container may fail to start within the script, such as for the InternVL 26B model. In such cases, try starting the container manually:

```bash
docker load -i <DOCKER_PATH>/vllm-openai-latest.tar -q
```

Run the container with appropriate GPU and port settings:

```bash
docker run --rm -it \
  --name internvl26b-vllm \
  --gpus all \
  --ipc=host \
  -p 8800:8000 \
  -v <HF_CACHE_PATH>:/root/.cache/huggingface \
  vllm/vllm-openai:latest \
  --model OpenGVLab/InternVL2_5-26B \
  --tokenizer-mode auto \
  --config-format auto \
  --load-format auto \
  --tool-call-parser qwen3_xml \
  --enable-auto-tool-choice \
  --tensor-parallel-size 2 \
  --dtype bfloat16 \
  --max-model-len 32768 \
  --trust-remote-code \
  --gpu-memory-utilization 0.95 \
  --max-num-seqs 12 \
  --max-num-batched-tokens 32768 \
  --kv-cache-memory-bytes 20000000000
  ```

Note: Replace `<HF_CACHE_PATH>` with your HuggingFace cache directory.
Parameters can be updated as needed. kv-cache-memory-bytes can be increased if you have more GPU memory available. The current setting uses 20GB of GPU memory for KV cache.

Once the model is loaded successfully, you should see a message like:
```
Application startup completed.
```
```
Set LOAD_DOCKER_CONTAINER to false in the config file to skip loading the container again in the script.

Open a new terminal, activate the venv and run the inference script:

```bash
source .venv/bin/activate
python main_docker.py data/configs/config_docker.json
```

## Memory Requirements

- **Qwen 32B**: ~150GB GPU memory
- **InternVL 26B**: ~145GB GPU memory
- **InternVL 8B**: ~35GB GPU memory

**Optimization tips:**
- Reduce `NUM_SEGMENTS` to lower memory usage
- Use key frames for variable frame counts
- Set `SAMPLE: true` for testing

## Troubleshooting

**Out of Memory**
- Reduce `NUM_SEGMENTS`
- Use smaller model variant
- Use key frames with lower threshold

**Model Not Found**
- Verify `MODEL_NAME` matches HuggingFace model ID
- Set `HF_TOKEN` for gated models: `export HF_TOKEN=your_token`

**Evaluation Skipped**
- Evaluation only runs if `CALCULATE_METRICS: true` AND `GROUND_TRUTH_FILE` exists
- Missing ground truth is not an error

**Failed Videos**
- Check `missed_videos.txt` for error details
- Common issues: corrupted videos, missing audio, unsupported formats

## Environment Variables

```bash
export HF_HOME=/path/to/cache          # HuggingFace cache
export HF_TOKEN=your_token             # For gated models
export CUDA_VISIBLE_DEVICES=0,1,2,3    # GPU selection
```

## Requirements

- Python 3.11+
- CUDA-capable GPU(s)
- Sufficient GPU memory (see Memory Requirements)
- Disk space for videos, frames, and outputs